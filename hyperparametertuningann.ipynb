{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the optimal number of hidden layers and neurons for an Artificial Neural Network (ANN) \n",
    "This can be challenging and often requires experimentation. However, there are some guidelines and methods that can help you in making an informed decision:\n",
    "\n",
    "- Start Simple: Begin with a simple architecture and gradually increase complexity if needed.\n",
    "- Grid Search/Random Search: Use grid search or random search to try different architectures.\n",
    "- Cross-Validation: Use cross-validation to evaluate the performance of different architectures.\n",
    "- Heuristics and Rules of Thumb: Some heuristics and empirical rules can provide starting points, such as:\n",
    "  -    The number of neurons in the hidden layer should be between the size of the input layer and the size of the output layer.\n",
    "  -  A common practice is to start with 1-2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Churn_Modelling.csv')\n",
    "data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "\n",
    "label_encoder_gender = LabelEncoder()\n",
    "data['Gender'] = label_encoder_gender.fit_transform(data['Gender'])\n",
    "\n",
    "onehot_encoder_geo = OneHotEncoder(handle_unknown='ignore')\n",
    "geo_encoded = onehot_encoder_geo.fit_transform(data[['Geography']]).toarray()\n",
    "geo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\n",
    "\n",
    "data = pd.concat([data.drop('Geography', axis=1), geo_encoded_df], axis=1)\n",
    "\n",
    "X = data.drop('Exited', axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save encoders and scaler for later use\n",
    "with open('label_encoder_gender.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder_gender, file)\n",
    "\n",
    "with open('onehot_encoder_geo.pkl', 'wb') as file:\n",
    "    pickle.dump(onehot_encoder_geo, file)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to create the model and try different parameters(KerasClassifier)\n",
    "\n",
    "def create_model(neurons=32,layers=1):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(neurons,activation='relu',input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(neurons,activation='relu'))\n",
    "\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Keras classifier\n",
    "model=KerasClassifier(layers=1,neurons=32,build_fn=create_model,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the grid search parameters\n",
    "param_grid = {\n",
    "    'neurons': [16, 32, 64, 128],\n",
    "    'layers': [1, 2],\n",
    "    'epochs': [50, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\andna\\ana3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\andna\\ana3\\Lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: Can't get attribute '_passthrough_scorer' on <module 'sklearn.metrics._scorer' from 'C:\\\\Users\\\\andna\\\\ana3\\\\Lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_scorer.py'>\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perform grid search\u001b[39;00m\n\u001b[0;32m      2\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print the best parameters\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (grid_result\u001b[38;5;241m.\u001b[39mbest_score_, grid_result\u001b[38;5;241m.\u001b[39mbest_params_))\n",
      "File \u001b[1;32m~\\ana3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_routed_params_for_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, params):\n\u001b[0;32m    869\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the parameters to be used for routing.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m \n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m    This is a method instead of a snippet in ``fit`` since it's used twice,\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m    here in ``fit``, and in ``HalvingRandomSearchCV.fit``.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n\u001b[0;32m    875\u001b[0m         routed_params \u001b[38;5;241m=\u001b[39m process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\ana3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGridSearchCV\u001b[39;00m(BaseSearchCV):\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Exhaustive search over specified parameter values for an estimator.\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \n\u001b[0;32m   1217\u001b[0m \u001b[38;5;124;03m    Important members are fit, predict.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \n\u001b[0;32m   1219\u001b[0m \u001b[38;5;124;03m    GridSearchCV implements a \"fit\" and a \"score\" method.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;124;03m    It also implements \"score_samples\", \"predict\", \"predict_proba\",\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    \"decision_function\", \"transform\" and \"inverse_transform\" if they are\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;124;03m    implemented in the estimator used.\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m \n\u001b[0;32m   1224\u001b[0m \u001b[38;5;124;03m    The parameters of the estimator used to apply these methods are optimized\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;124;03m    by cross-validated grid-search over a parameter grid.\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m \n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <grid_search>`.\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m \n\u001b[0;32m   1229\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;124;03m    estimator : estimator object\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;124;03m        This is assumed to implement the scikit-learn estimator interface.\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;124;03m        Either estimator needs to provide a ``score`` function,\u001b[39;00m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;124;03m        or ``scoring`` must be passed.\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \n\u001b[0;32m   1236\u001b[0m \u001b[38;5;124;03m    param_grid : dict or list of dictionaries\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;124;03m        Dictionary with parameters names (`str`) as keys and lists of\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;124;03m        parameter settings to try as values, or a list of such\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m        dictionaries, in which case the grids spanned by each dictionary\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;124;03m        in the list are explored. This enables searching over any sequence\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;124;03m        of parameter settings.\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m \n\u001b[0;32m   1243\u001b[0m \u001b[38;5;124;03m    scoring : str, callable, list, tuple or dict, default=None\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m        Strategy to evaluate the performance of the cross-validated model on\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m        the test set.\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \n\u001b[0;32m   1247\u001b[0m \u001b[38;5;124;03m        If `scoring` represents a single score, one can use:\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m \n\u001b[0;32m   1249\u001b[0m \u001b[38;5;124;03m        - a single string (see :ref:`scoring_parameter`);\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;124;03m        - a callable (see :ref:`scoring_callable`) that returns a single value.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \n\u001b[0;32m   1252\u001b[0m \u001b[38;5;124;03m        If `scoring` represents multiple scores, one can use:\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m \n\u001b[0;32m   1254\u001b[0m \u001b[38;5;124;03m        - a list or tuple of unique strings;\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;124;03m        - a callable returning a dictionary where the keys are the metric\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;124;03m          names and the values are the metric scores;\u001b[39;00m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;124;03m        - a dictionary with metric names as keys and callables as values.\u001b[39;00m\n\u001b[0;32m   1258\u001b[0m \n\u001b[0;32m   1259\u001b[0m \u001b[38;5;124;03m        See :ref:`multimetric_grid_search` for an example.\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \n\u001b[0;32m   1261\u001b[0m \u001b[38;5;124;03m    n_jobs : int, default=None\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;124;03m        Number of jobs to run in parallel.\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;124;03m        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;124;03m        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;124;03m        for more details.\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m \n\u001b[0;32m   1267\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: v0.20\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;124;03m           `n_jobs` default changed from 1 to None\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m \n\u001b[0;32m   1270\u001b[0m \u001b[38;5;124;03m    refit : bool, str, or callable, default=True\u001b[39;00m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;124;03m        Refit an estimator using the best found parameters on the whole\u001b[39;00m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;124;03m        dataset.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m        For multiple metric evaluation, this needs to be a `str` denoting the\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m        scorer that would be used to find the best parameters for refitting\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;124;03m        the estimator at the end.\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m \n\u001b[0;32m   1278\u001b[0m \u001b[38;5;124;03m        Where there are considerations other than maximum score in\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m        choosing a best estimator, ``refit`` can be set to a function which\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m        returns the selected ``best_index_`` given ``cv_results_``. In that\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;124;03m        case, the ``best_estimator_`` and ``best_params_`` will be set\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;124;03m        according to the returned ``best_index_`` while the ``best_score_``\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;124;03m        attribute will not be available.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03m        The refitted estimator is made available at the ``best_estimator_``\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;124;03m        attribute and permits using ``predict`` directly on this\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;124;03m        ``GridSearchCV`` instance.\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m \n\u001b[0;32m   1289\u001b[0m \u001b[38;5;124;03m        Also for multiple metric evaluation, the attributes ``best_index_``,\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;124;03m        ``best_score_`` and ``best_params_`` will only be available if\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;124;03m        ``refit`` is set and all of them will be determined w.r.t this specific\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;124;03m        scorer.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m        See ``scoring`` parameter to know more about multiple metric\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;124;03m        evaluation.\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \n\u001b[0;32m   1297\u001b[0m \u001b[38;5;124;03m        See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;124;03m        to see how to design a custom selection strategy using a callable\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;124;03m        via `refit`.\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m \n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.20\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;124;03m            Support for callable added.\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m \n\u001b[0;32m   1304\u001b[0m \u001b[38;5;124;03m    cv : int, cross-validation generator or an iterable, default=None\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;124;03m        Determines the cross-validation splitting strategy.\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;124;03m        Possible inputs for cv are:\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \n\u001b[0;32m   1308\u001b[0m \u001b[38;5;124;03m        - None, to use the default 5-fold cross validation,\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;124;03m        - integer, to specify the number of folds in a `(Stratified)KFold`,\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;124;03m        - :term:`CV splitter`,\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;124;03m        - An iterable yielding (train, test) splits as arrays of indices.\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \n\u001b[0;32m   1313\u001b[0m \u001b[38;5;124;03m        For integer/None inputs, if the estimator is a classifier and ``y`` is\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;124;03m        either binary or multiclass, :class:`StratifiedKFold` is used. In all\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;124;03m        other cases, :class:`KFold` is used. These splitters are instantiated\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;124;03m        with `shuffle=False` so the splits will be the same across calls.\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m \n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m        Refer :ref:`User Guide <cross_validation>` for the various\u001b[39;00m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;124;03m        cross-validation strategies that can be used here.\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m \n\u001b[0;32m   1321\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.22\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;124;03m            ``cv`` default value if None changed from 3-fold to 5-fold.\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \n\u001b[0;32m   1324\u001b[0m \u001b[38;5;124;03m    verbose : int\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;124;03m        Controls the verbosity: the higher, the more messages.\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m        - >1 : the computation time for each fold and parameter candidate is\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m          displayed;\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m        - >2 : the score is also displayed;\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m        - >3 : the fold and candidate parameter indexes are also displayed\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m          together with the starting time of the computation.\u001b[39;00m\n\u001b[0;32m   1332\u001b[0m \n\u001b[0;32m   1333\u001b[0m \u001b[38;5;124;03m    pre_dispatch : int, or str, default='2*n_jobs'\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;124;03m        Controls the number of jobs that get dispatched during parallel\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m        execution. Reducing this number can be useful to avoid an\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m        explosion of memory consumption when more jobs get dispatched\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m        than CPUs can process. This parameter can be:\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m        - None, in which case all the jobs are immediately created and spawned. Use\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m          this for lightweight and fast-running jobs, to avoid delays due to on-demand\u001b[39;00m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;124;03m          spawning of the jobs\u001b[39;00m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;124;03m        - An int, giving the exact number of total jobs that are spawned\u001b[39;00m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;124;03m        - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\u001b[39;00m\n\u001b[0;32m   1344\u001b[0m \n\u001b[0;32m   1345\u001b[0m \u001b[38;5;124;03m    error_score : 'raise' or numeric, default=np.nan\u001b[39;00m\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;124;03m        Value to assign to the score if an error occurs in estimator fitting.\u001b[39;00m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;124;03m        If set to 'raise', the error is raised. If a numeric value is given,\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;124;03m        FitFailedWarning is raised. This parameter does not affect the refit\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;124;03m        step, which will always raise the error.\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m \n\u001b[0;32m   1351\u001b[0m \u001b[38;5;124;03m    return_train_score : bool, default=False\u001b[39;00m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;124;03m        If ``False``, the ``cv_results_`` attribute will not include training\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;124;03m        scores.\u001b[39;00m\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;124;03m        Computing training scores is used to get insights on how different\u001b[39;00m\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;124;03m        parameter settings impact the overfitting/underfitting trade-off.\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;124;03m        However computing the scores on the training set can be computationally\u001b[39;00m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;124;03m        expensive and is not strictly required to select the parameters that\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;124;03m        yield the best generalization performance.\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m \n\u001b[0;32m   1360\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.19\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m \n\u001b[0;32m   1362\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.21\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;124;03m            Default value was changed from ``True`` to ``False``\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m \n\u001b[0;32m   1365\u001b[0m \u001b[38;5;124;03m    Attributes\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;124;03m    cv_results_ : dict of numpy (masked) ndarrays\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;124;03m        A dict with keys as column headers and values as columns, that can be\u001b[39;00m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;124;03m        imported into a pandas ``DataFrame``.\u001b[39;00m\n\u001b[0;32m   1370\u001b[0m \n\u001b[0;32m   1371\u001b[0m \u001b[38;5;124;03m        For instance the below given table\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m \n\u001b[0;32m   1373\u001b[0m \u001b[38;5;124;03m        +------------+-----------+------------+-----------------+---+---------+\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;124;03m        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;124;03m        +============+===========+============+=================+===+=========+\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;124;03m        |  'poly'    |     --    |      2     |       0.80      |...|    2    |\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;124;03m        +------------+-----------+------------+-----------------+---+---------+\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m        |  'poly'    |     --    |      3     |       0.70      |...|    4    |\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;124;03m        +------------+-----------+------------+-----------------+---+---------+\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;124;03m        |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\u001b[39;00m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;124;03m        +------------+-----------+------------+-----------------+---+---------+\u001b[39;00m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;124;03m        |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\u001b[39;00m\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;124;03m        +------------+-----------+------------+-----------------+---+---------+\u001b[39;00m\n\u001b[0;32m   1384\u001b[0m \n\u001b[0;32m   1385\u001b[0m \u001b[38;5;124;03m        will be represented by a ``cv_results_`` dict of::\u001b[39;00m\n\u001b[0;32m   1386\u001b[0m \n\u001b[0;32m   1387\u001b[0m \u001b[38;5;124;03m            {\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m \u001b[38;5;124;03m            'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;124;03m                                         mask = [False False False False]...)\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;124;03m            'param_gamma': masked_array(data = [-- -- 0.1 0.2],\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;124;03m                                        mask = [ True  True False False]...),\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;124;03m            'param_degree': masked_array(data = [2.0 3.0 -- --],\u001b[39;00m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;124;03m                                         mask = [False False  True  True]...),\u001b[39;00m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;124;03m            'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\u001b[39;00m\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;124;03m            'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\u001b[39;00m\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;124;03m            'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\u001b[39;00m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;124;03m            'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;124;03m            'rank_test_score'    : [2, 4, 3, 1],\u001b[39;00m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;124;03m            'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\u001b[39;00m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;124;03m            'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\u001b[39;00m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m            'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;124;03m            'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\u001b[39;00m\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;124;03m            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;124;03m            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\u001b[39;00m\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;124;03m            'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\u001b[39;00m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;124;03m            'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\u001b[39;00m\n\u001b[0;32m   1407\u001b[0m \u001b[38;5;124;03m            'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\u001b[39;00m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m            }\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03m        NOTE\u001b[39;00m\n\u001b[0;32m   1411\u001b[0m \n\u001b[0;32m   1412\u001b[0m \u001b[38;5;124;03m        The key ``'params'`` is used to store a list of parameter\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;124;03m        settings dicts for all the parameter candidates.\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \n\u001b[0;32m   1415\u001b[0m \u001b[38;5;124;03m        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\u001b[39;00m\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;124;03m        ``std_score_time`` are all in seconds.\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m \n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m        For multi-metric evaluation, the scores for all the scorers are\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m        available in the ``cv_results_`` dict at the keys ending with that\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;124;03m        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;124;03m        above. ('split0_test_precision', 'mean_train_precision' etc.)\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m \n\u001b[0;32m   1423\u001b[0m \u001b[38;5;124;03m    best_estimator_ : estimator\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;124;03m        Estimator that was chosen by the search, i.e. estimator\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m \u001b[38;5;124;03m        which gave highest score (or smallest loss if specified)\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;124;03m        on the left out data. Not available if ``refit=False``.\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m \n\u001b[0;32m   1428\u001b[0m \u001b[38;5;124;03m        See ``refit`` parameter for more information on allowed values.\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m \n\u001b[0;32m   1430\u001b[0m \u001b[38;5;124;03m    best_score_ : float\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;124;03m        Mean cross-validated score of the best_estimator\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m \n\u001b[0;32m   1433\u001b[0m \u001b[38;5;124;03m        For multi-metric evaluation, this is present only if ``refit`` is\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m \u001b[38;5;124;03m        specified.\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m \n\u001b[0;32m   1436\u001b[0m \u001b[38;5;124;03m        This attribute is not available if ``refit`` is a function.\u001b[39;00m\n\u001b[0;32m   1437\u001b[0m \n\u001b[0;32m   1438\u001b[0m \u001b[38;5;124;03m    best_params_ : dict\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;124;03m        Parameter setting that gave the best results on the hold out data.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m \n\u001b[0;32m   1441\u001b[0m \u001b[38;5;124;03m        For multi-metric evaluation, this is present only if ``refit`` is\u001b[39;00m\n\u001b[0;32m   1442\u001b[0m \u001b[38;5;124;03m        specified.\u001b[39;00m\n\u001b[0;32m   1443\u001b[0m \n\u001b[0;32m   1444\u001b[0m \u001b[38;5;124;03m    best_index_ : int\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;124;03m        The index (of the ``cv_results_`` arrays) which corresponds to the best\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03m        candidate parameter setting.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03m        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\u001b[39;00m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;124;03m        the parameter setting for the best model, that gives the highest\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;124;03m        mean score (``search.best_score_``).\u001b[39;00m\n\u001b[0;32m   1451\u001b[0m \n\u001b[0;32m   1452\u001b[0m \u001b[38;5;124;03m        For multi-metric evaluation, this is present only if ``refit`` is\u001b[39;00m\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;124;03m        specified.\u001b[39;00m\n\u001b[0;32m   1454\u001b[0m \n\u001b[0;32m   1455\u001b[0m \u001b[38;5;124;03m    scorer_ : function or a dict\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m        Scorer function used on the held out data to choose the best\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m        parameters for the model.\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m \n\u001b[0;32m   1459\u001b[0m \u001b[38;5;124;03m        For multi-metric evaluation, this attribute holds the validated\u001b[39;00m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;124;03m        ``scoring`` dict which maps the scorer key to the scorer callable.\u001b[39;00m\n\u001b[0;32m   1461\u001b[0m \n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m    n_splits_ : int\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m        The number of cross-validation splits (folds/iterations).\u001b[39;00m\n\u001b[0;32m   1464\u001b[0m \n\u001b[0;32m   1465\u001b[0m \u001b[38;5;124;03m    refit_time_ : float\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;124;03m        Seconds used for refitting the best model on the whole dataset.\u001b[39;00m\n\u001b[0;32m   1467\u001b[0m \n\u001b[0;32m   1468\u001b[0m \u001b[38;5;124;03m        This is present only if ``refit`` is not False.\u001b[39;00m\n\u001b[0;32m   1469\u001b[0m \n\u001b[0;32m   1470\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m \n\u001b[0;32m   1472\u001b[0m \u001b[38;5;124;03m    multimetric_ : bool\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;124;03m        Whether or not the scorers compute several metrics.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \n\u001b[0;32m   1475\u001b[0m \u001b[38;5;124;03m    classes_ : ndarray of shape (n_classes,)\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;124;03m        The classes labels. This is present only if ``refit`` is specified and\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;124;03m        the underlying estimator is a classifier.\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m \n\u001b[0;32m   1479\u001b[0m \u001b[38;5;124;03m    n_features_in_ : int\u001b[39;00m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;124;03m        Number of features seen during :term:`fit`. Only defined if\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;124;03m        `best_estimator_` is defined (see the documentation for the `refit`\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;124;03m        parameter for more details) and that `best_estimator_` exposes\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;124;03m        `n_features_in_` when fit.\u001b[39;00m\n\u001b[0;32m   1484\u001b[0m \n\u001b[0;32m   1485\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.24\u001b[39;00m\n\u001b[0;32m   1486\u001b[0m \n\u001b[0;32m   1487\u001b[0m \u001b[38;5;124;03m    feature_names_in_ : ndarray of shape (`n_features_in_`,)\u001b[39;00m\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;124;03m        Names of features seen during :term:`fit`. Only defined if\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;124;03m        `best_estimator_` is defined (see the documentation for the `refit`\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;124;03m        parameter for more details) and that `best_estimator_` exposes\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;124;03m        `feature_names_in_` when fit.\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \n\u001b[0;32m   1493\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.0\u001b[39;00m\n\u001b[0;32m   1494\u001b[0m \n\u001b[0;32m   1495\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;124;03m    ParameterGrid : Generates all the combinations of a hyperparameter grid.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;124;03m    train_test_split : Utility function to split the data into a development\u001b[39;00m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;124;03m        set usable for fitting a GridSearchCV instance and an evaluation set\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;124;03m        for its final evaluation.\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;124;03m    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;124;03m        loss function.\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \n\u001b[0;32m   1504\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;124;03m    -----\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;124;03m    The parameters selected are those that maximize the score of the left out\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;124;03m    data, unless an explicit score is passed in which case it is used instead.\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m \n\u001b[0;32m   1509\u001b[0m \u001b[38;5;124;03m    If `n_jobs` was set to a value higher than one, the data is copied for each\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;124;03m    point in the grid (and not `n_jobs` times). This is done for efficiency\u001b[39;00m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;124;03m    reasons if individual jobs take very little time, but may raise errors if\u001b[39;00m\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;124;03m    the dataset is large and not enough memory is available.  A workaround in\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m \u001b[38;5;124;03m    this case is to set `pre_dispatch`. Then, the memory is copied only\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;124;03m    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;124;03m    n_jobs`.\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \n\u001b[0;32m   1517\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m   1518\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;124;03m    >>> from sklearn import svm, datasets\u001b[39;00m\n\u001b[0;32m   1520\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.model_selection import GridSearchCV\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;124;03m    >>> iris = datasets.load_iris()\u001b[39;00m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;124;03m    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;124;03m    >>> svc = svm.SVC()\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;124;03m    >>> clf = GridSearchCV(svc, parameters)\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;124;03m    >>> clf.fit(iris.data, iris.target)\u001b[39;00m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;124;03m    GridSearchCV(estimator=SVC(),\u001b[39;00m\n\u001b[0;32m   1527\u001b[0m \u001b[38;5;124;03m                 param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\u001b[39;00m\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;124;03m    >>> sorted(clf.cv_results_.keys())\u001b[39;00m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;124;03m    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\u001b[39;00m\n\u001b[0;32m   1530\u001b[0m \u001b[38;5;124;03m     'param_C', 'param_kernel', 'params',...\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;124;03m     'rank_test_score', 'split0_test_score',...\u001b[39;00m\n\u001b[0;32m   1532\u001b[0m \u001b[38;5;124;03m     'split2_test_score', ...\u001b[39;00m\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;124;03m     'std_fit_time', 'std_score_time', 'std_test_score']\u001b[39;00m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m     _parameter_constraints: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1537\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mBaseSearchCV\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[0;32m   1538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_grid\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mlist\u001b[39m],\n\u001b[0;32m   1539\u001b[0m     }\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m   1542\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1543\u001b[0m         estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1554\u001b[0m     ):\n",
      "File \u001b[1;32m~\\ana3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    810\u001b[0m multimetric_refit_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor multi-metric scoring, the parameter refit must be set to a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscorer key or a callable to refit an estimator with the best \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m )\n\u001b[0;32m    819\u001b[0m valid_refit_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit \u001b[38;5;129;01min\u001b[39;00m scores\n\u001b[1;32m--> 821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_refit_dict\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit)\n\u001b[0;32m    825\u001b[0m ):\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(multimetric_refit_msg)\n",
      "File \u001b[1;32m~\\ana3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterable):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch the tasks and return the results.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    iterable : iterable\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        Iterable containing tuples of (delayed_function, args, kwargs) that should\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        be consumed.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    results : list\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m        List of results of the tasks.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     config \u001b[38;5;241m=\u001b[39m get_config()\n",
      "File \u001b[1;32m~\\ana3\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\ana3\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\ana3\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32m~\\ana3\\Lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\ana3\\Lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\ana3\\Lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3,verbose=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
